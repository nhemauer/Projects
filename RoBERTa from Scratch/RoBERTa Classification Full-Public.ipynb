{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796a5d-df4f-4829-bc1f-d8ef0aae6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e492d7-0d92-444c-aed9-6a9efda20ff7",
   "metadata": {},
   "source": [
    "# Creating and Implementing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f44a43-8a29-43fa-b364-237924d4ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, config_path):\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "\n",
    "    \n",
    "    \n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "    \n",
    "    def decode(self, tokens, skip_special_tokens=False):\n",
    "        if isinstance(tokens, list):\n",
    "            return self.tokenizer.decode(tokens)\n",
    "        elif isinstance(tokens, torch.Tensor):\n",
    "            return self.tokenizer.decode(tokens.tolist())\n",
    "        else:\n",
    "            raise ValueError(\"Tokens must be a list or torch.Tensor\")\n",
    "            \n",
    "        if skip_special_tokens:\n",
    "            for token in self.special_tokens:\n",
    "                decoded = decoded.replace(token, '')\n",
    "            decoded = ' '.join(decoded.split())  # Remove extra spaces\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokenizer.convert_tokens_to_ids(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def get_eos_token_id(self):\n",
    "        return self.tokenizer.eos_token_id\n",
    "    \n",
    "    def get_bos_token_id(self):\n",
    "        return self.tokenizer.bos_token_id\n",
    "    \n",
    "    def get_pad_token_id(self):\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        return self.tokenizer.mask_token_id\n",
    "    \n",
    "    \n",
    "    \n",
    "class Encoding(Tokenizer):\n",
    "    def __init__(self, prompt):\n",
    "        super().__init__(tokenizer_path)\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    def enc(self):\n",
    "        encoded = self.tokenizer.encode(self.prompt)\n",
    "        tokens = encoded.ids\n",
    "        bos_token = \"<s>\"\n",
    "        bos_id = self.tokenizer.token_to_id(bos_token)\n",
    "        if bos_id is not None:\n",
    "            tokens = [bos_id] + tokens\n",
    "        else:\n",
    "            print(f\"Error: '{bos_token}' token not found in vocabulary. This should not happen.\")\n",
    "            print(\"Vocabulary:\", self.tokenizer.get_vocab())\n",
    "        \n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    \n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.dim = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"] \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.embedding_layer.weight.data.copy_(self.model.embeddings.word_embeddings.weight)\n",
    "\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "    \n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, seq_length):\n",
    "        positions = torch.arange(seq_length, device=self.pos_embedding.weight.device)\n",
    "        return self.pos_embedding(positions)\n",
    "\n",
    "    \n",
    "    \n",
    "class RobertaLikeModel(nn.Module):\n",
    "    def __init__(self, model_loader, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = model_loader.get_config()\n",
    "        self.pretrained_model = model_loader.get_model()\n",
    "        self.encoder = self.pretrained_model.encoder\n",
    "        \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.word_embedding = Embedding(self.pretrained_model, self.config)\n",
    "        self.positional_embedding = PositionalEmbedding(self.config)\n",
    "        \n",
    "        self.attention = self.pretrained_model.encoder.layer[0].attention\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(self.config[\"hidden_size\"], num_labels)\n",
    "        )\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(self.config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, prompts):                                          # nn.Forward runs automatically when you apply it to input data\n",
    "        # Handle both single string and list of strings\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        # Encode the prompts\n",
    "        encoded = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        \n",
    "        # Pad sequences to the same length\n",
    "        max_len = max(len(seq) for seq in encoded)\n",
    "        padded = [seq + [self.tokenizer.get_pad_token_id()] * (max_len - len(seq)) for seq in encoded]\n",
    "        \n",
    "        # Convert to tensor and add batch dimension if necessary\n",
    "        input_ids = torch.tensor(padded)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        \n",
    "        # Word embeddings\n",
    "        word_embeds = self.word_embedding.get_embeddings(input_ids)\n",
    "        \n",
    "        # Positional embeddings\n",
    "        pos_embeds = self.positional_embedding(input_ids.size(1))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = word_embeds + pos_embeds\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = (input_ids != self.tokenizer.get_pad_token_id()).float()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # Pass through all encoder layers\n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=extended_attention_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        # Use the [CLS] token representation for classification\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        # Pass through the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892be6f-95f9-4ac3-bd9d-2a4e98795944",
   "metadata": {},
   "source": [
    "# Defining CustomDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2855af-f1c5-4e4f-bcdd-af310bdaa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Add input validation\n",
    "        if not isinstance(text, str):\n",
    "            #print(f\"Warning: Non-string input detected at index {idx}. Input: {text}\")\n",
    "            text = str(text)  # Convert to string\n",
    "\n",
    "        # We don't need to tokenize here because the model does it internally\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': torch.tensor(label, dtype=torch.long).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1d0a7-2a3d-4ce7-8463-a77b60379022",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec375-e1d9-471d-a10d-c62ea0519bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\", encoding='latin-1')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "np.random.seed(1339)\n",
    "\n",
    "# Creates the dataframe # List are the labels\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54444041-4688-45ed-8f27-c68182c2f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8 # 80% Train Size\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset.comment_text, train_dataset.list, Tokenizer._load_tokenizer(\"a\"))\n",
    "val_dataset = CustomDataset(test_dataset.comment_text, test_dataset.list, Tokenizer._load_tokenizer(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522472d8-2ef7-4d24-a283-2aa194cc5150",
   "metadata": {},
   "source": [
    "# Defining Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397646a0-3196-43ea-8bed-cc9a71bc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, num_epochs = 10, batch_size = 8, learning_rate = 1e-5):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = batch_size, drop_last = True)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            prompts = batch['text']\n",
    "            labels = batch['label']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(prompts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                prompts = batch['text']\n",
    "                labels = batch['label']\n",
    "\n",
    "                outputs = model(prompts)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = torch.sigmoid(outputs) > 0.5  # Apply sigmoid and thresholding; this is a hyperparameter\n",
    "                \n",
    "                correct_predictions += torch.sum(preds == labels).item()\n",
    "                total_predictions += labels.numel()  # Total number of elements\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "        print()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8bf06-9dbe-4fe2-94bb-1c4b18c54d33",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeeb5d-cdd2-46f5-a4bf-93d86a2b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and RoBERTa Config\n",
    "config_path = \"config.json\" # RoBERTa config from Github\n",
    "model_loader = ModelLoader(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaLikeModel(model_loader, num_labels = 13)\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(model, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
