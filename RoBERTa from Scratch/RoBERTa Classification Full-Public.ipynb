{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796a5d-df4f-4829-bc1f-d8ef0aae6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7c33b-fa22-4933-b8ff-ccc98857c25a",
   "metadata": {},
   "source": [
    "# Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db1a4a-1ff7-4a4d-bec0-7ab746495ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e492d7-0d92-444c-aed9-6a9efda20ff7",
   "metadata": {},
   "source": [
    "# Creating and Implementing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f44a43-8a29-43fa-b364-237924d4ab24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, config_path):\n",
    "        self.model = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.config = self._load_config(config_path)\n",
    "        \n",
    "    def _load_config(self, config_path):\n",
    "        with open(config_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_config(self):\n",
    "        return self.config\n",
    "\n",
    "    \n",
    "    \n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "    \n",
    "    def _load_tokenizer(self):\n",
    "        # Load the tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "        \n",
    "        return tokenizer\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode(text)\n",
    "    \n",
    "    def decode(self, tokens, skip_special_tokens=False):\n",
    "        if isinstance(tokens, list):\n",
    "            return self.tokenizer.decode(tokens)\n",
    "        elif isinstance(tokens, torch.Tensor):\n",
    "            return self.tokenizer.decode(tokens.tolist())\n",
    "        else:\n",
    "            raise ValueError(\"Tokens must be a list or torch.Tensor\")\n",
    "            \n",
    "        if skip_special_tokens:\n",
    "            for token in self.special_tokens:\n",
    "                decoded = decoded.replace(token, '')\n",
    "            decoded = ' '.join(decoded.split())  # Remove extra spaces\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        return self.tokenizer.convert_tokens_to_ids(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.tokenizer)\n",
    "    \n",
    "    def get_eos_token_id(self):\n",
    "        return self.tokenizer.eos_token_id\n",
    "    \n",
    "    def get_bos_token_id(self):\n",
    "        return self.tokenizer.bos_token_id\n",
    "    \n",
    "    def get_pad_token_id(self):\n",
    "        return self.tokenizer.pad_token_id\n",
    "\n",
    "    def get_mask_token_id(self):\n",
    "        return self.tokenizer.mask_token_id\n",
    "    \n",
    "    \n",
    "    \n",
    "class Encoding(Tokenizer):\n",
    "    def __init__(self, prompt):\n",
    "        super().__init__(tokenizer_path)\n",
    "        self.prompt = prompt\n",
    "        \n",
    "    def enc(self):\n",
    "        encoded = self.tokenizer.encode(self.prompt)\n",
    "        tokens = encoded.ids\n",
    "        bos_token = \"<s>\"\n",
    "        bos_id = self.tokenizer.token_to_id(bos_token)\n",
    "        if bos_id is not None:\n",
    "            tokens = [bos_id] + tokens\n",
    "        else:\n",
    "            print(f\"Error: '{bos_token}' token not found in vocabulary. This should not happen.\")\n",
    "            print(\"Vocabulary:\", self.tokenizer.get_vocab())\n",
    "        \n",
    "        return torch.tensor(tokens)\n",
    "\n",
    "    \n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.dim = config[\"hidden_size\"]\n",
    "        self.vocab_size = config[\"vocab_size\"] \n",
    "        self.embedding_layer = nn.Embedding(self.vocab_size, self.dim)\n",
    "        self.embedding_layer.weight.data.copy_(self.model.embeddings.word_embeddings.weight)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_layer.to(self.device)\n",
    "\n",
    "    def get_embeddings(self, tokens):\n",
    "        return self.embedding_layer(tokens).to(torch.bfloat16)\n",
    "\n",
    "    \n",
    "    \n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(config[\"max_position_embeddings\"], config[\"hidden_size\"])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, seq_length):\n",
    "        positions = torch.arange(seq_length, device=self.device)\n",
    "        return self.pos_embedding(positions)\n",
    "\n",
    "    \n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(config[\"hidden_size\"] / config[\"num_attention_heads\"])\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.key = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "        self.value = nn.Linear(config[\"hidden_size\"], self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        self.dense = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"])\n",
    "        self.LayerNorm = nn.LayerNorm(config[\"hidden_size\"], eps=config[\"layer_norm_eps\"])\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / torch.sqrt(torch.tensor(self.attention_head_size, dtype=torch.float))\n",
    "        \n",
    "        # Apply the attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        attention_output = self.dense(context_layer)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = self.LayerNorm(attention_output + hidden_states)\n",
    "\n",
    "        return attention_output\n",
    "    \n",
    "    \n",
    "    \n",
    "class RobertaLikeModel(nn.Module):\n",
    "    def __init__(self, model_loader, num_labels):\n",
    "        super().__init__()\n",
    "        self.config = model_loader.get_config()\n",
    "        self.pretrained_model = model_loader.get_model()\n",
    "        self.encoder = self.pretrained_model.encoder\n",
    "        \n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.word_embedding = Embedding(self.pretrained_model, self.config)\n",
    "        self.positional_embedding = PositionalEmbedding(self.config)\n",
    "        \n",
    "        self.attention = MultiHeadAttention(self.config)\n",
    "        # This can be replaced by the following line to make use of the pretrained attention weights:\n",
    "        self.attention = self.pretrained_model.encoder.layer[0].attention\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.config[\"hidden_dropout_prob\"]),\n",
    "            nn.Linear(self.config[\"hidden_size\"], num_labels)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, prompts):\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        encoded = [self.tokenizer.encode(prompt) for prompt in prompts]\n",
    "        max_len = max(len(seq) for seq in encoded)\n",
    "        padded = [seq + [self.tokenizer.get_pad_token_id()] * (max_len - len(seq)) for seq in encoded]\n",
    "        \n",
    "        input_ids = torch.tensor(padded, device=self.device)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        \n",
    "        word_embeds = self.word_embedding.get_embeddings(input_ids)\n",
    "        pos_embeds = self.positional_embedding(input_ids.size(1))\n",
    "        \n",
    "        embeddings = word_embeds + pos_embeds\n",
    "        \n",
    "        attention_mask = (input_ids != self.tokenizer.get_pad_token_id()).float()\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        encoder_outputs = self.encoder(embeddings, attention_mask=extended_attention_mask)\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        \n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892be6f-95f9-4ac3-bd9d-2a4e98795944",
   "metadata": {},
   "source": [
    "# Defining CustomDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2855af-f1c5-4e4f-bcdd-af310bdaa12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Add input validation\n",
    "        if not isinstance(text, str):\n",
    "            #print(f\"Warning: Non-string input detected at index {idx}. Input: {text}\")\n",
    "            text = str(text)  # Convert to string\n",
    "\n",
    "        # We don't need to tokenize here because the model does it internally\n",
    "        return {\n",
    "            'text': text,\n",
    "            'label': torch.tensor(label, dtype=torch.long).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1d0a7-2a3d-4ce7-8463-a77b60379022",
   "metadata": {},
   "source": [
    "# Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bec375-e1d9-471d-a10d-c62ea0519bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\", encoding='latin-1')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "np.random.seed(1339)\n",
    "\n",
    "# Creates the dataframe # List are the labels\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "# Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54444041-4688-45ed-8f27-c68182c2f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8 # 80% Train Size\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "train_dataset = CustomDataset(train_dataset.comment_text, train_dataset.list, Tokenizer._load_tokenizer(\"a\"))\n",
    "val_dataset = CustomDataset(test_dataset.comment_text, test_dataset.list, Tokenizer._load_tokenizer(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522472d8-2ef7-4d24-a283-2aa194cc5150",
   "metadata": {},
   "source": [
    "# Defining Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397646a0-3196-43ea-8bed-cc9a71bc75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, val_dataset, num_epochs=10, batch_size=8, learning_rate=1e-5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            prompts = batch['text']\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(prompts)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                prompts = batch['text']\n",
    "                labels = batch['label'].to(device)\n",
    "                outputs = model(prompts)\n",
    "                \n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                correct_predictions += torch.sum(preds == labels).item()\n",
    "                total_predictions += labels.numel()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "        print()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8bf06-9dbe-4fe2-94bb-1c4b18c54d33",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdeeb5d-cdd2-46f5-a4bf-93d86a2b02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and RoBERTa Config\n",
    "config_path = \"config.json\" # RoBERTa config from Github\n",
    "model_loader = ModelLoader(config_path)\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaLikeModel(model_loader, num_labels = 13).to(get_device())\n",
    "\n",
    "# Train model\n",
    "trained_model = train_model(model, train_dataset, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
