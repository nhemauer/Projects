{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4db338-6d95-490f-b10c-d9f68832f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c0a56d2-0cab-456a-a292-325f044f76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from transformers import DebertaV2Model\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e89a8e-0c9f-4383-a89d-3a45c424807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92aeba-5b59-4e2a-bd0f-c6e31350807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\", encoding='latin-1')\n",
    "df = pd.DataFrame(df)\n",
    "df = df.iloc[: , 1:]\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "#Creates the dataframe\n",
    "df['list'] = df[df.columns[2:]].values.tolist()\n",
    "new_df = df[['CASEID', 'comment_text', 'list']].copy()\n",
    "\n",
    "#Applies float to list\n",
    "new_df['list'] = new_df['list'].apply(lambda x: [float(i) for i in x])\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c853982-6a87-4343-8392-181d890bf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 8 #8\n",
    "VALID_BATCH_SIZE = 4 #4\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "#Defining Tokenizer\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69595131-ddb0-4971-a28b-5353fb27ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.comment_text\n",
    "        self.targets = self.data.list\n",
    "        self.CASEID = self.data.CASEID\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'caseid': self.CASEID[index],\n",
    "            'text': comment_text,\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b13c5f5-276d-4e3a-b690-6639892c834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (13987, 3)\n",
      "TRAIN Dataset: (11190, 3)\n",
      "TEST Dataset: (2797, 3)\n"
     ]
    }
   ],
   "source": [
    "#Creating the dataset and dataloader for the neural network\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset=new_df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15af4425-5697-4995-90a6-98f35ee0ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53ac7d09-4fb1-411d-9433-1484b4c87289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEBERTAClass(\n",
       "  (l1): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (l2): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating the customized model by adding dropout.\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# class BERTClass(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(BERTClass, self).__init__()\n",
    "#         self.l1 = transformers.RobertaModel.from_pretrained('roberta-base')\n",
    "#         self.l2 = torch.nn.Dropout(0.2)\n",
    "#         self.l3 = torch.nn.Linear(768, 13)  # Num of labels is 13\n",
    "    \n",
    "#     def forward(self, ids, mask, token_type_ids):\n",
    "#         _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "#         output_2 = self.l2(output_1)\n",
    "#         output = self.l3(output_2)\n",
    "#         return output\n",
    "\n",
    "class DEBERTAClass(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.l1 = DebertaV2Model.from_pretrained('microsoft/deberta-v3-base')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.layer_norm = nn.LayerNorm(self.l1.config.hidden_size)\n",
    "        self.l2 = nn.Linear(self.l1.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        outputs = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs[0]  # Get the last hidden state\n",
    "        \n",
    "        # Pooling: Use the [CLS] token representation (first token)\n",
    "        pooled_output = last_hidden_state[:, 0, :]\n",
    "        \n",
    "        output_2 = self.dropout(pooled_output)\n",
    "        output_3 = self.layer_norm(output_2)\n",
    "        output = self.l2(output_3)\n",
    "        return output    \n",
    "    \n",
    "    \n",
    "model = DEBERTAClass(13)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de82c9f0-a61f-45ae-bc03-6cd3475e585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets) #This is binary cross entropy in keras I believe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acf0eb36-41c1-41df-9186-a4bab82d63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02aa3735-4e7d-4446-a17e-00e012f68dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined training function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch in tqdm.tqdm(training_loader, desc=f\"Epoch {epoch}\"):\n",
    "        # Unpack the batch\n",
    "        ids = batch['ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = batch['targets'].to(device, dtype=torch.float)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b850352-2242-44be-982b-c9c0d6eaf655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1399/1399 [03:04<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.10601064562797546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1399/1399 [03:05<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.04568728432059288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1399/1399 [03:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.020508555695414543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1399/1399 [03:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.0027236351743340492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1399/1399 [03:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0.0027477932162582874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1399/1399 [03:03<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0.06260765343904495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss: 0.0037785936146974564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss: 0.008677409961819649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss: 0.008691200986504555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1399/1399 [03:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 0.05751186981797218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.006522975862026215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss: 0.0009502205066382885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1399/1399 [03:04<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss: 0.0479586236178875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1399/1399 [03:04<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss: 0.0043518138118088245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1399/1399 [03:03<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss: 0.0012626174138858914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Training Function\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7fe4130f-a6fc-412d-8688-4c9584723eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Function\n",
    "def validation(epoch):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    texts = []\n",
    "    caseid = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(testing_loader, desc=f\"Validation Epoch {epoch}\"):\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.float)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            texts.extend(batch['text'])  # Extracting the text column\n",
    "            caseid.extend(batch['caseid'])\n",
    "    \n",
    "    return fin_outputs, fin_targets, texts, caseid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74210d7d-9616-41d9-a09d-5ec65bd82d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 0:   0%|          | 0/700 [00:00<?, ?it/s]/storage/home/ndh5286/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Validation Epoch 0: 100%|██████████| 700/700 [00:14<00:00, 48.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.8902395423668216\n",
      "F1 Score (Micro) = 0.9174342656171497\n",
      "F1 Score (Macro) = 0.7432268256014265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Metrics function\n",
    "for epoch in range(1):\n",
    "    outputs, targets, texts, caseid = validation(epoch)\n",
    "    outputs = np.array(outputs) >= 0.65                                      #This can be tuned\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    #print(\"First three items of texts:\", texts[:3])\n",
    "    #print(\"First three items of fin_outputs:\", outputs[:3])\n",
    "    #print(\"First three items of fin_targets:\", targets[:3])\n",
    "    \n",
    "    #Create a dictionary with column names as keys and lists as values\n",
    "    final_df = {\n",
    "        'Outputs': outputs,\n",
    "        'Targets': targets,\n",
    "        'Texts': texts,\n",
    "        'Caseid': caseid\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "716703af-b96c-49bd-b540-5a1387098f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating final output\n",
    "final_case = final_df['Caseid']\n",
    "final_case = pd.DataFrame(final_case, columns = [\"Caseid\"])\n",
    "\n",
    "final_text = final_df['Texts']\n",
    "final_text = pd.DataFrame(final_text, columns = [\"Text\"])\n",
    "\n",
    "final_output = final_df['Outputs']\n",
    "final_output = pd.DataFrame(final_output, columns = [\"Freedom and Rights\", \"E Plurabus Unum\", \"Representation of the People\", \"Popular Will and Equality\", \"National Identity and Heritage\", \"Not a Democracy a Republic\", \n",
    "                \"Flawed Democracy\", \"Institution and Constitution\", \"Unclassified/Other\", \"Don't Know\", \"Nothing/Disaffected\", \"Nothing More to Add\", \"NA\"])\n",
    "final_output.replace({True: 1, False: 0}, inplace=True)\n",
    "\n",
    "final_df = final_text.join(final_output)\n",
    "final_df = final_case.join(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb6ac3-8a6c-4282-82aa-9d0d95cca77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e995014-7073-48c7-a7c2-d3efd5187100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Freedom and Rights:\n",
      "[[2214   37]\n",
      " [  28  518]]\n",
      "Confusion matrix for E Plurabus Unum:\n",
      "[[2775    7]\n",
      " [  10    5]]\n",
      "Confusion matrix for Representation of the People:\n",
      "[[2704   26]\n",
      " [  20   47]]\n",
      "Confusion matrix for Popular Will and Equality:\n",
      "[[2219   47]\n",
      " [  27  504]]\n",
      "Confusion matrix for National Identity and Heritage:\n",
      "[[2788    4]\n",
      " [   4    1]]\n",
      "Confusion matrix for Not a Democracy a Republic:\n",
      "[[2761    2]\n",
      " [   0   34]]\n",
      "Confusion matrix for Flawed Democracy:\n",
      "[[2710   21]\n",
      " [  19   47]]\n",
      "Confusion matrix for Institution and Constitution:\n",
      "[[2684   21]\n",
      " [  32   60]]\n",
      "Confusion matrix for Unclassified/Other:\n",
      "[[2594   28]\n",
      " [  59  116]]\n",
      "Confusion matrix for Don't Know:\n",
      "[[2748    4]\n",
      " [   2   43]]\n",
      "Confusion matrix for Nothing/Disaffected:\n",
      "[[2720   32]\n",
      " [  10   35]]\n",
      "Confusion matrix for Nothing More to Add:\n",
      "[[2592   13]\n",
      " [  24  168]]\n",
      "Confusion matrix for NA:\n",
      "[[1620   16]\n",
      " [   0 1161]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "true_np = targets\n",
    "pred_np = outputs\n",
    "\n",
    "#Calculate confusion matrix\n",
    "conf_matrix = multilabel_confusion_matrix(true_np, pred_np)\n",
    "\n",
    "#Define label titles\n",
    "label_titles = [\"Freedom and Rights\", \"E Plurabus Unum\", \"Representation of the People\", \"Popular Will and Equality\", \"National Identity and Heritage\", \"Not a Democracy a Republic\", \n",
    "                \"Flawed Democracy\", \"Institution and Constitution\", \"Unclassified/Other\", \"Don't Know\", \"Nothing/Disaffected\", \"Nothing More to Add\", \"NA\"]\n",
    "\n",
    "#Create a dictionary to store confusion matrices with titles\n",
    "conf_matrix_dict = {}\n",
    "for i, title in enumerate(label_titles):\n",
    "    conf_matrix_dict[title] = conf_matrix[i]\n",
    "\n",
    "#Print confusion matrices with titles\n",
    "for title, matrix in conf_matrix_dict.items():\n",
    "    print(f\"Confusion matrix for {title}:\")\n",
    "    print(matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
